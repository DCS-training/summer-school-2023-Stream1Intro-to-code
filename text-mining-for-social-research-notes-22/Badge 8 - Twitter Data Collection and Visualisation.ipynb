{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook:\n",
    "\n",
    "1. [Connecting to the Twitter API](#1)\n",
    "2. [Searching for a specific user](#2)\n",
    "3. [Searching for a specific topic](#3)\n",
    "4. [Extending the search and working with multi-level JSON Data](#4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "# 1. Connecting to the Twitter API\n",
    "\n",
    "## Questions & Objectives\n",
    "\n",
    "* Setting up access and validity signing\n",
    "* Setting up a handler to manage the connection\n",
    "* Running a test search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will download the libraries that deal with accessing the API (`tweepy`) and working with the JSON data (`json`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run this cell now to import the libraries.\n",
    "\n",
    "!pip install tweepy\n",
    "import tweepy        # https://github.com/tweepy/tweepy\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then set up the variables that hold the validation keys. You need to add your keys (tokens) and secrets in the spaces below. Make sure to put them between the speech marks and make sure there are no extra spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your keys and secrets and then run this cell.\n",
    "\n",
    "access_key = ''\n",
    "access_secret = ''\n",
    "api_key = ''\n",
    "api_secret = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we set up the authication handler. We pass the keys and secrets as below and then set up the API object. We can use this object to connect to the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(api_key, api_secret)\n",
    "auth.set_access_token(access_key, access_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the connection we will run a test query.\n",
    "\n",
    "We use the API object and we are going to ask for some of the tweets from users you follow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_tweets = api.home_timeline()\n",
    "for tweet in public_tweets:\n",
    "    print(tweet.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "# 2. Searching for a Specific User\n",
    "\n",
    "* Search for a specific user\n",
    "* Retrieve data from the Twitter API\n",
    "* Call specific items from the JSON data object\n",
    "* Look at the full JSON data\n",
    "\n",
    "We will now look for tweets from a specific person. To do this we need their Twitter name. If you go to https://twitter.com/BarackObama you can see the Twitter name under the main name. You can see it has an @ sign in front that we remove from our code.  \n",
    "\n",
    "For this we use the `get_user` method from the Twitter API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we create a variable, call the information on the user Barack Obama, and hold it\n",
    "# in the variable we created.\n",
    "\n",
    "user = api.get_user(screen_name='BarackObama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This object is in JSON tuples.\n",
    "# We can call the tuples and print their content. \n",
    "# We will look more at JSON later.\n",
    "# We can print the screen name as below:\n",
    "\n",
    "print(user.screen_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can print the number of followers:\n",
    "\n",
    "print(user.followers_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can print the user description:\n",
    "\n",
    "print(user.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see all of the user information in its raw format we can type:\n",
    "\n",
    "print(user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üêõMinitask\n",
    "\n",
    "* Try using the information from the user to print out to access the other information.\n",
    "* See if you can work out how to get to the nested tuples.\n",
    "* Try and look at another user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can get tweets from the API user timeline.\n",
    "# This time we call the user_timeline method again with the BarackObama user method.\n",
    "# Here we call the last two tweets.\n",
    "# These are retured in a list object.\n",
    "\n",
    "new_tweets = api.user_timeline(screen_name = 'BarackObama', count = 2)   # replace BarackObama with another user's name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we can tweet the first tweet (which remember is 0 in a list).\n",
    "# What other information can you access from the tweets?  How about the number of retweets?\n",
    "\n",
    "new_tweets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "# 3. Searching for a Topic\n",
    "\n",
    "* Search the Twitter API using a keyword\n",
    "* Retrieve the text from a single tweet\n",
    "* Retrieve the text from multiple tweets\n",
    "* Process and clean the text\n",
    "* Visualise the text\n",
    "\n",
    "We will now look for tweets that contain a specific word. \n",
    "\n",
    "For this we use the `search` method from the Twitter API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are looking for the word covid.\n",
    "# We are asking for 10 english tweets to be returned.\n",
    "# They are returned as a list.\n",
    "\n",
    "covid_tweets = api.search_tweets(q='covid', lang='en', count='10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can print out the first tweet in the list.\n",
    "\n",
    "covid_tweets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we can't just call the JSON from the object (like we did with the user object).\n",
    "We have to deal with the JSON directly. We do this using the `_json` function.\n",
    "Then we can call all of the tuples as a dictionary object. \n",
    "\n",
    "(Remember a tuple takes the form `['text':'this is tweet text']`, which means that we can retrieve the content of the tuple by the key of the tuple.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we can see all of the json in a nice format...\n",
    "\n",
    "covid_tweets[0]._json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...or we can just call the text.\n",
    "\n",
    "covid_tweets[0]._json['text'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can text put the text into its own list and just work with just the text.\n",
    "\n",
    "tweets_text = []\n",
    "for each in covid_tweets:\n",
    "    tweets_text.append(each._json['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see how we have put the tweets' text into a list.\n",
    "\n",
    "print(tweets_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can treat the tweets' text like we did in earlier badges,\n",
    "# for example, we can turn it into a string and tokenise it.\n",
    "\n",
    "tweets_string = \" \".join(tweets_text)\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(tweets_string)\n",
    "print(tokens[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can clean up the tweets' text like we did earlier, making it all lowercase and removing stop words.\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "lowercase_tokens = [token.lower() for token in tokens]\n",
    "remove_these = set(stopwords.words('english') + list(string.punctuation) + list(string.digits))\n",
    "filtered_text = [token \n",
    "                 for token in lowercase_tokens \n",
    "                 if not token in remove_these]\n",
    "print(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can calculate word frequencies...\n",
    "\n",
    "from collections import Counter\n",
    "simple_frequencies_dict = Counter(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...and produce word clouds.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "cloud = WordCloud(width=800, \n",
    "                  height=400, \n",
    "                  max_font_size=160, \n",
    "                  colormap=\"hsv\").generate_from_frequencies(simple_frequencies_dict)\n",
    "plt.figure(figsize=(16,12))\n",
    "plt.imshow(cloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üêõMinitask\n",
    "\n",
    "* Try using a visualisation method or a search method you have used before to visualise the text.\n",
    "* Try searching for a different word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "## 4. Extending the search and working with multi-level JSON Data\n",
    "\n",
    "* Search the Twitter API using an extended query with multiple terms\n",
    "* Search using a tweepy cursor to retrieve more data\n",
    "* Look at nested data in the JSON\n",
    "\n",
    "We will now look for tweets that contain several words. We can combine query words with the operator `OR`. We can use this operator to say, give me tweets that contain `word1` or `word2`. You might want to do this with related words on the same topic, or with multiple spellings or potential typos of a word. \n",
    "\n",
    "For this we will continue to use the `search` method from the Twitter API.\n",
    "\n",
    "We want to gather more data than we did before. The `search` method limits the data we can retrieve. To extend the amount of data we retrieve we use a tweepy `Cursor`. Twitter returns multiple pages of data, almost like a book, but it will only give you one page at a time. Before, we only took the first page. This time, we will page through the extended version using a `Cursor` object. The `Cursor` maintains the connection with the API and allows us to ask for the next page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We set up a list to hold the tweets so we can then append to it as we iterate through the pages.\n",
    "# Previously, we created a list in the search, but here we need to create a list so we can add to it.\n",
    "\n",
    "covid_tweets = []\n",
    "\n",
    "# We set up a tweepy Cursor to maintain the connection.\n",
    "# We set up the query with the OR operator.\n",
    "# We iterate through the pages from the API using a for loop.\n",
    "# We append the content to a list.\n",
    "for page in tweepy.Cursor(api.search_tweets, \n",
    "                          q='covid OR covid19 OR COVID OR COVID19 or #covid', \n",
    "                          lang='en').pages(10):\n",
    "    covid_tweets.append(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_tweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see the text from the first tweet:\n",
    "\n",
    "print(covid_tweets[0][0].text) # covid_tweets[0][0] is the first Status (tweet) object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twitter data is nested.\n",
    "\n",
    "This means that it can contain items within items. \n",
    "\n",
    "For example hashtags, user mentions, and URLs are contained within an `entities` dictionary.\n",
    "\n",
    "This looks like:\n",
    "\n",
    "```\n",
    "'entities': { \n",
    "    'hashtags': [{'hashtag1'}, {'hashtag2'}], \n",
    "    'user_mentions': [{'screen_name':'barackobama', 'name': 'Barack Obama'}], \n",
    "    'urls': [{'url':'www.bbc.co.uk'}]\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The hashtags are contained in a list within the entity tuple,\n",
    "# which means we need to call the entity tuple (hashtag) and then iterate through the list.\n",
    "# We set up a list to hold the hashtags so we can then append to it as we iterate.\n",
    "# We iterate through each tweet, and then through the hashtags in the list,\n",
    "# adding the tweets to the list.\n",
    "\n",
    "covid_hashtags = []\n",
    "for search_result in covid_tweets:\n",
    "    for status in search_result:  # for every tweet\n",
    "        hashtags = status.entities['hashtags']\n",
    "        if len(hashtags) > 0:     # if there are hashtags\n",
    "            for h in hashtags:\n",
    "                covid_hashtags.append(h['text'])\n",
    "\n",
    "print(covid_hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can then visualise these hashtags in the ways we learnt before.\n",
    "\n",
    "hashtag_string = \" \".join(covid_hashtags)\n",
    "tokens = word_tokenize(hashtag_string)\n",
    "simple_frequencies_dict_covid = Counter(tokens)\n",
    "cloud = WordCloud(width=800, height=400, max_font_size=160, \n",
    "                  colormap=\"viridis\", \n",
    "                  background_color='white',).generate_from_frequencies(simple_frequencies_dict_covid)\n",
    "plt.figure(figsize=(16,12))\n",
    "plt.imshow(cloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üêõMinitask\n",
    "\n",
    "* Try creating a visualisation with a different nested item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Have this as a task -- look for another item of interest, maybe alter to be URLs?\n",
    "\n",
    "# covid_mentions = []\n",
    "# for search_result in covid_tweets:\n",
    "#     for status in search_result:\n",
    "#         mention = status._json['entities']['user_mentions']\n",
    "#         if len(mention) > 0:\n",
    "#             i = 0\n",
    "#             while i < len(mention):\n",
    "#                 covid_mentions.append(mention[i]['name'])\n",
    "#                 i += 1\n",
    "# people_dict=Counter(covid_mentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cloud = WordCloud(width=800, height=400, max_font_size=200,\n",
    "#                   background_color='white', colormap=\"viridis\").generate_from_frequencies(people_dict)\n",
    "# plt.figure(figsize=(16,12))\n",
    "# plt.imshow(cloud, interpolation='bilinear')\n",
    "# plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# covid_tweets = []\n",
    "# for page in tweepy.Cursor(api.search, q='brexit', lang='en', min_retweets=\"1000\").pages(100):\n",
    "#     covid_tweets.append(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(covid_tweets))\n",
    "# print(covid_tweets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
