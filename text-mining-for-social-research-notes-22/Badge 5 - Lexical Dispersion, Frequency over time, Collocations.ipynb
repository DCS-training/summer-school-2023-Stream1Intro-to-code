{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook:\n",
    "\n",
    "1. [**Lexical Dispersion Plot** - where in the corpus a word appears](#1)\n",
    "2. [Plotting **Frequency Over Time**](#2)\n",
    "3. [**Collocations** of Words - when words appear frequently near each other](#3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "# 1. Lexical Dispersion Plot - where in the corpus a word appears\n",
    "\n",
    "\n",
    "#### Questions & Objectives\n",
    "\n",
    "- How can I measure how frequently a word appears across a corpus?\n",
    "- How can I use a DataFrame?\n",
    "- How can I plot the occurrences of a word and its location(s) (its word number(s) counting from the beginning of the corpus), called word offsets?\n",
    "- We will use the a dataset from the Scottish Parliament.\n",
    "\n",
    "#### Key Points\n",
    "\n",
    "- Lexical dispersion is a visualisation that allows us to see where a particular term appears across a document or corpus (set of documents).\n",
    "- We can use NLTK’s `dispersion_plot` to visualise lexical dispersion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell now. It's the usual imports of text mining libraries.\n",
    "import nltk\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot lexical dispersion of particular tokens.\n",
    "\n",
    "**Lexical dispersion is a measure of how frequently a word appears across the parts of a corpus**. \n",
    "\n",
    "This plot notes the occurrences of a word and its word offsets (its location counting from the beginning of the corpus). This is particularly useful for a corpus that covers a long time period and for which you want to analyse how specific terms were used more or less frequently over time.\n",
    "\n",
    "To create a lexical disperson plot, you will first load and import a different corpus, which is a transcript of session at the Scottish Parliament held from January 2020. This dataset is a subset of the parlScot dataset made available by Harvard University (Braby, D. and Stewart, F., 2021, \"parlScot: a dataset of 1.8 million spoken contributions from the Scottish Parliament 1999-2021\", https://doi.org/10.7910/DVN/EQ9WBE, Harvard Dataverse, V1). The COVID-19 specific subset, which we use, contains all (6,152) parliamentary speeches which mention covid or coronavirus in them. You can find it in the data directory of this course on Noteable (data > scotParl > parlScot_parl_v1.1-covid.csv).\n",
    "\n",
    "This data is stored in CSV (comma-separated values) format which means that each speech along with metadata about the speech (e.g., the speaker’s name, political party, etc.) are all stored on one line and each piece of information is separated by a comma.  You can think of CSV format as if it is a table where each row includes information about a speech and each column refers to the same type of information (for example, the speech itself or the speaker’s name).\n",
    "\n",
    "Many libraries you will use (for text mining, visualisation, etc.) come with built-in datasets for you to practice with. They are nicely structured in this way. We will be using the Pandas library to import the CSV and retain the structure that would have been seen in the spreadsheet (rows and columns). It is particularly useful for working with big, multi-format datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we import Pandas so we can use its DataFrame format to hold the data.\n",
    "# The data we have doesn't have the column headings included but these are avaliable in the schema notes \n",
    "# at this URL: https://dataverse.harvard.edu/file.xhtml?fileId=4432890&version=1.0\n",
    "# As they aren't there when we read in the CSV we say it has no header (=0) and we add in the names of \n",
    "# the columns.\n",
    "# Take a look at the heading and see if you can work out what they mean. Take a look at the schema notes \n",
    "# to see if you are right.\n",
    "import pandas as pd\n",
    "df = pd.read_csv('./data/scotParl/parlScot_parl_v1.1-covid.csv', \n",
    "            header=0, \n",
    "            names=['x', 'x2','day_order','order','is_speech','committee','date','item','type','office_held','display_as','name','speech','parl_id','party','gender','constituency','region','msp_tyoe','wikidataid','party.facts.id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The DataFrame is too big to look at all in one go but we can see the top of it using the df.head() function\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or the bottom of it using the df.tail() function\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As this is records of parliament sessions we have various columns that hold different information\n",
    "# We can count the number of rows in a specific column\n",
    "# Here we look at item which is defined in the schema as \"Focus of Debate\"\n",
    "df['item'].count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can look at the unique versions of that field. So all the differnt \"Focus of Debate\"\n",
    "# Here we can count them\n",
    "df['item'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and here we can list them\n",
    "df['item'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can count the number of times each one appears\n",
    "df['item'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can do this with other fields such as the date field \n",
    "df['date'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and we can call specific columns -- for example here we look at the transcripts of the speeches given\n",
    "# Note how you don't ses all 6150 rows. We get given the head and the tail of the file -- like we had before\n",
    "# Also because the content is long it is trucated -- we don't see the full speech just the first few words. \n",
    "# This allows us to check it has gone in OK and is what we expected without having to see the full thing\n",
    "df['speech']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we wanted to see the full speeches we can use the .tolist() function.\n",
    "# This copies that column from the DataFrame into a list.\n",
    "speeches = df['speech'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we print the first speech. Remember, the first item in the list starts at position zero.\n",
    "speeches[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can tokenise these speeches like we have done before.\n",
    "# In this case we first join all the speeches to create one large string containing them all.\n",
    "s = \" \".join(speeches)\n",
    "s_tokens = word_tokenize(s) \n",
    "lower_s_tokens = [word.lower() for word in s_tokens] \n",
    "print(lower_s_tokens[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use a stoplist and a list of punctuation and digits to be removed.\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "remove_these = set(stopwords.words('english') + list(string.punctuation) + list(string.digits))\n",
    "\n",
    "filtered_text = [token \n",
    "                 for token in s_tokens \n",
    "                 if not token in remove_these]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered_text[0:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the lexical dispersion plot for this corpus you also need to load `dispersion_plot` from the `nltk.draw.dispersion` package.\n",
    "\n",
    "You can then call the `dispersion_plot` method given a set of parameters, including the target words you want to plot across the corpus, whether this should be done case-sensitively, and the title of the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We download the appropriate library\n",
    "from nltk.draw.dispersion import dispersion_plot\n",
    "\n",
    "# The following command can be used to increase the size of the plot using width and height specifications\n",
    "plt.figure(figsize=(12, 9))\n",
    "\n",
    "# Set the words we wish to look for as targets \n",
    "targets=['coronavirus','mask','wash','hands','lockdown','economy']\n",
    "\n",
    "# and create the plot\n",
    "dispersion_plot(filtered_text, targets, ignore_case=True, title='Lexical Dispersion Plot for the Covid-19 Scottish Parliament Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🖇🐛 Thinking Minitask: What words might have been used only in some time periods?\n",
    "\n",
    "- Adjust the above code to include other words. Remember these are speeches in the Scottish parliament from January 2020 until February 2021. What words might have appeared over certain periods and not others? Try words like 'vaccine'.\n",
    "\n",
    "Do not spend more than 2 minutes on this. Just try some words and move on. Things will get even more interesting in a minute.\n",
    "\n",
    "Notice that it is really annoying that we cannot see the exact date when a particular word was heavily used, we can only see when a word appeared across the whole corpus. We will solve that problem in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary style='color:blue'>CLICK HERE TO SEE THE THE ANSWER. BUT REALLY TRY TO DO IT YOURSELF FIRST!</summary>\n",
    "\n",
    "    ### BEGIN SOLUTION\n",
    "    Change the contents of targets=['vaccine','furlough','ppe','lateral','app'] above to include your chosen words and re-run that code cell.\n",
    "         \n",
    "### END SOLUTION\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "# 2. Plotting Frequency Over Time\n",
    "\n",
    "\n",
    "#### Questions & Objectives\n",
    "\n",
    "- How can I extract and plot the frequency of specific terms over time?\n",
    "- How can I use NLTK’s `ConditionalFreqDist` class to extract the frequency of defined words?\n",
    "\n",
    "#### Key Points\n",
    "\n",
    "- We extract terms and the years from the files using NLTK’s `ConditionalFreqDist` class from the `nltk.probability` package.\n",
    "- We plot these on a graph to visualise how the those terms' usage changes over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nested Loops: a new, challenging Python syntax\n",
    "\n",
    "This is a new Python syntax for loops inside of loops (nested loops), which is VERY CHALLENGING.\n",
    "\n",
    "Do not worry if you do not get it at first (don't spend more than 2 minutes on this), just move on to the next tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell and then read through it. \n",
    "    \n",
    "# Goal: we have a set of fruit names and a set of target letters.\n",
    "# Each time a fruit contains a target letter, return them.\n",
    "# E.g., because 'pear' contains 'a' and 'p' return [('pear', 'p'), ('pear', 'a')].\n",
    "\n",
    "fruits = ['pear', \"banana\", \"kiwi\", 'apple' ]\n",
    "targets = ['a', 'p', 'w']\n",
    "\n",
    "new_words = [(fruit, target)\n",
    "            for fruit in fruits\n",
    "            for letter in fruit\n",
    "            for target in targets\n",
    "            if letter == target\n",
    "            ]\n",
    "print(new_words)\n",
    "\n",
    "# If this syntax is not clear, ask your buddy 🖇, but even if it is not super clear,\n",
    "# you'll be fine, just continue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to take meta-information from files to understand a corpus better\n",
    "\n",
    "Similar to lexical dispersion, you can also plot the frequency of terms over time. This is also similar to the [Google n-gram](https://books.google.com/ngrams) visualisation for the Google Books corpus; we will show you how to do something similar for your own corpus.\n",
    "\n",
    "You first need to import NLTK’s `ConditionalFreqDist` class from the `nltk.probability` package. To generate the graph, you have to specify the list of words to be plotted (see targets) and the x-axis labels (in this case, the date column from the DataFrame which we looked at earlier).\n",
    "\n",
    "The required data for the plot needs to be in a format where a word is repeated for each date as many times as it was used in that speech.\n",
    "\n",
    "```\n",
    "[('coronavirus', '20-01-23'),\n",
    " ('coronavirus', '20-01-28'),\n",
    " ('coronavirus', '20-02-20'),\n",
    " ('coronavirus', '20-02-26'),\n",
    " ('coronavirus', '20-03-03'),\n",
    " ('coronavirus', '20-03-04'),\n",
    "...\n",
    "```\n",
    "\n",
    "To create this dataset, we:\n",
    "\n",
    "- return a tuple with a word and the date of each speech `(target, row['date'])`\n",
    "- for each row in the DataFrame: `for x, row in df.iterrows()`\n",
    "- then for each **word** in that speech `for word in word_tokenize(row['speech'])`\n",
    "- then for each **target** word in our specified target words\n",
    "- use that word **only if** the word starts with the target `if word.lower().startswith(target))`\n",
    "    \n",
    "```\n",
    "    [(target, row['date'])\n",
    "    for x, row in df.iterrows()\n",
    "    for word in word_tokenize(row['speech'])  \n",
    "    for target in targets\n",
    "    if word.lower().startswith(target)])\n",
    "\n",
    "```\n",
    "    \n",
    "\n",
    "The `ConditionalFreqDist` object (cfd) stores the number of times each of the target words appear in the each of the speeches and the plot() method is used to visualise the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import ConditionalFreqDist\n",
    "\n",
    "# The next line of code sets the figure size\n",
    "plt.rcParams[\"figure.figsize\"] = (24, 9)\n",
    "\n",
    "targets=['coronavirus','mask','lockdown','economy']\n",
    "#for x, row in df.iterrows():\n",
    "#    print(x, row['date'])\n",
    "\n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "    [(target, row['date'])\n",
    "    for x, row in df.iterrows()\n",
    "    for word in word_tokenize(row['speech'])  \n",
    "    for target in targets\n",
    "    if word.lower().startswith(target)])\n",
    "\n",
    "cfd.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🐛Minitask: \n",
    "\n",
    "- Change the words in the above graph. Use the words you discussed with your buddy above.\n",
    "\n",
    "- Try to use Regular Expressions instead of specific words (see hints below).\n",
    "\n",
    "E.g., if you want to compare occurences of:\n",
    "\n",
    "- the words `covid & covid-19`\n",
    "- the word `virus`\n",
    "- any other words that contain `virus` \n",
    "\n",
    "you could use targets:\n",
    "\n",
    "`targets=['^covid...$', '^virus$', 'virus$']`\n",
    "\n",
    "and instead of:\n",
    "\n",
    "`if word.lower().startswith(target)])`\n",
    "\n",
    "use:\n",
    "\n",
    "`if re.search(target, word.lower()))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy-paste the graph code to this cell and write your answer here\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<details><summary style='color:blue'>CLICK HERE TO SEE THE THE ANSWER. BUT REALLY TRY TO DO IT YOURSELF FIRST!</summary>\n",
    "\n",
    "    ### BEGIN SOLUTION\n",
    "    plt.rcParams[\"figure.figsize\"] = (12, 9)\n",
    "    targets=['^m[ea]n$', '^freedom$', '^free']\n",
    "    cfd = nltk.ConditionalFreqDist((target, fileid[:4])\n",
    "        for fileid in inaugural.fileids()\n",
    "        for word in inaugural.words(fileid)\n",
    "        for target in targets\n",
    "        if re.search(target, word.lower()))\n",
    "    cfd.plot()\n",
    "         \n",
    "### END SOLUTION\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "# 3. Collocations of Words - when words appear frequently near each other\n",
    "\n",
    "\n",
    "#### Questions & Objectives\n",
    "\n",
    "- How can I see what terms are often used together in a text or corpus?\n",
    "- We want to see words that collocate, meaning they occur together more often than they would by chance.\n",
    "- We will see what words co-occur within 5 words of one another.\n",
    "- We will then see which words appear more than 10 times together.\n",
    "- We will then look at a measure to score the likelihood of these collocations being unusual (occurring together more often than they would by chance).\n",
    "\n",
    "#### Key Points\n",
    "\n",
    "- We will use NLTK’s `BigramAssocMeasures()` and `BigramCollocationFinder` to find the words commonly found together in the COVID-19-related Scottish Parliament speeches.\n",
    "- We will score these collocations using the `bigram_measures.likelihood_ratio`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may want to see what terms are often used together. We can do this by looking for collocations in a text, meaning two word tokens occurring together in the text more often than would be expected by chance.\n",
    "\n",
    "For this we need to import the `nltk.collocations` module and, more specifically, `BigramAssocMeasures()` and `BigramCollocationFinder`. We allow a window of 5 words between collocated words.\n",
    "\n",
    "Note this next bit of code takes a couple of minutes to run, so wait until it's done before you proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import BigramAssocMeasures\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "\n",
    "bigram_measures = BigramAssocMeasures()\n",
    "tokens = []\n",
    "for x, row in df.iterrows():\n",
    "    tokens = tokens + word_tokenize(row['speech'])\n",
    "\n",
    "string_tokens = \" \".join(tokens)\n",
    "finder = BigramCollocationFinder.from_words(tokens, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then look for words that appear together 10 times or more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finder.apply_freq_filter(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A number of measures are available to score collocations or other associations including `bigram_measures.likelihood_ratio`. We apply this measure below and show the top ten collocated tokens (occuring in a window of 5 tokens with a frequency of 30 or more occurrences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finder.nbest(bigram_measures.likelihood_ratio, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also just look at the speeches that were specifically focused on COVID-19 \n",
    "# (and didn't just mention it) to find more relevent content.\n",
    "tokens = []\n",
    "for x, row in df.iterrows():\n",
    "    if \"Covid-19\" in row['item']:\n",
    "        tokens = tokens + word_tokenize(row['speech'])\n",
    "\n",
    "string_tokens = \" \".join(tokens)\n",
    "finder = BigramCollocationFinder.from_words(tokens, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finder.apply_freq_filter(10)\n",
    "finder.nbest(bigram_measures.likelihood_ratio, 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🐛Minitask:  Re-do the collocation analysis after removing stop words, punctuation marks, etc.\n",
    "\n",
    "Change the code below to display collocations in the inaugural speeches with these additional requirements:\n",
    "\n",
    "- All tokens in the `inaugural_tokens` are lowercased\n",
    "- Stop words, punctuation and single digits are removed from the `inaugural_tokens`\n",
    "\n",
    "Refer back to previous notebook for help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for x, row in df.iterrows():\n",
    "    if \"Covid-19\" in row['item']:\n",
    "        tokens = tokens + word_tokenize(row['speech'])\n",
    "\n",
    "\n",
    "# HERE you will want to filter inaugural_tokens to exclude stop words, punctuation, and single digits.\n",
    "\n",
    "# Write your code here....\n",
    "\n",
    "bigram_measures = BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(tokens, 5)\n",
    "finder.apply_freq_filter(10)\n",
    "finder.nbest(bigram_measures.likelihood_ratio, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details><summary style='color:blue'>CLICK HERE TO SEE THE THE ANSWER. BUT REALLY TRY TO DO IT YOURSELF FIRST!</summary>\n",
    "    \n",
    "    nltk.download('stopwords')\n",
    "    from nltk.corpus import stopwords\n",
    "    remove_these = set(stopwords.words('english') + list(string.punctuation) + list(string.digits))\n",
    "    \n",
    "    tokens = []\n",
    "    for x, row in df.iterrows():\n",
    "    \n",
    "        if \"Covid-19\" in row['item']:\n",
    "            tokens = tokens + word_tokenize(row['speech'])\n",
    "\n",
    "    # HERE you will want to filter inaugural_tokens to not contain stopwords, punctuation, etc \n",
    "\n",
    "    ### BEGIN SOLUTION\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "\n",
    "    tokens = [word\n",
    "                  for word in tokens \n",
    "                  if not word in remove_these]\n",
    "\n",
    "    ### END SOLUTION\n",
    "\n",
    "    bigram_measures = BigramAssocMeasures()\n",
    "    finder = BigramCollocationFinder.from_words(tokens, 5)\n",
    "    finder.apply_freq_filter(10)\n",
    "    finder.nbest(bigram_measures.likelihood_ratio, 30)\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
