{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d8eb4a6",
   "metadata": {},
   "source": [
    "#  Finally: Practical things to do with Numpy and Panda\n",
    "\n",
    "\n",
    "### - in this notebook:\n",
    "\n",
    "- loading files: local (from your machine) and remote (accessed with online url)\n",
    "- cleaning up the missing values and manipulating data\n",
    "- visualisations with numpy or matplotlib\n",
    "- examples of the whole journey of prepping simple data to Visualisation\n",
    "- mini-diary ⭐️⭐️⭐️❓\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba95e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38499d6f",
   "metadata": {},
   "source": [
    "# Importing data from files\n",
    "\n",
    "There are two main datatypes you will encounter.\n",
    "\n",
    "**CSV (comma separated values)** - those basically look like simplified excel. First line of the file contains names of columns, and then each next line is a set of values for those columns. Valyes are separated by comma.\n",
    "\n",
    "`\n",
    "name, age, department\n",
    "Pim, 34, ER\n",
    "Jannet, 54, Oncology\n",
    "Aoife, 25, Oncology\n",
    "`\n",
    "\n",
    "Notice you cannot skip any values, since the only indicator of what data means is which in order it is. So for example, if you do not know one person's name you can't just write\n",
    "\n",
    "Natasha, ER\n",
    "\n",
    "becuase it would treat ER as their age. (second value means second column). Instead sometimes you will see missing data just skipped with a missing value (nothing between commas) like this:\n",
    "\n",
    "Natasha,, ER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d315fce3",
   "metadata": {},
   "source": [
    "**JSON (key-value pairs, like python dictionary)** - those will be very familiar to you. Collecitons are indicated by [ ] and key value pairs (separated by : colon) are enclosed in { }\n",
    "\n",
    "`\n",
    "[\n",
    "    {'name':'Pim', 'age':34, 'department':'Oncology'},\n",
    "    {'name':'Jannet', 'age':54, 'department':'Oncology'},\n",
    "    {'name':'Aoife', 'age':25, 'department':'Oncology'}\n",
    "]\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e6b384",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b1f638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import some data from a csv file. Panda simplified and streamlined importing data\n",
    "\n",
    "# if we did not specify index_col, index would be 0,1,2,3,... but this data already has a gooid index \n",
    "nursing_home_residents = pd.read_csv(\"./data/nursing_home_residents.csv\")\n",
    "nursing_home_residents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5846f6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and we could creater an index, but in this dataset nont of the columns is uniquely identifying a row\n",
    "# instead what is unique is the combination of date, statistic and CA (council area)\n",
    "\n",
    "# so eg IF the date was a unique row item, we could use index_col to specify index\n",
    "# nursing_home_residents = pd.read_csv(\"../data/nursing_home_residents.csv\", index_col='Date')\n",
    "# nursing_home_residents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4104242",
   "metadata": {},
   "source": [
    "### Online files use EXACTLY the same code\n",
    "\n",
    "This is just so amazing. if the 'filename' you use in read_csv starts with http... it will be simply loaded from the internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6764ca00",
   "metadata": {},
   "outputs": [],
   "source": [
    "nursing_home_residents = pd.read_csv(\"https://www.opendata.nhs.scot/dataset/75cca0a9-780d-40e0-9e1f-5f4796950794/resource/139f61d8-a87d-419d-b7af-31f555a60c89/download/file3_mean_median_age_years.csv\")\n",
    "nursing_home_residents\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e8255b",
   "metadata": {},
   "source": [
    "### Json Files: similar, but might need some specifying if JSON is very nested (things, within things within things)\n",
    "\n",
    "For example, if the file `simple.json` just contains a list of things, like this:\n",
    "\n",
    "`\n",
    "[\n",
    "    {\"name\":\"Pim\", \"age\":34, \"department\":\"Oncology\"},\n",
    "    {\"name\":\"Jannet\", \"age\":54, \"department\":\"Oncology\"},\n",
    "    {\"name\":\"Aoife\", \"age\":25, \"department\":\"Oncology\"}\n",
    "]\n",
    "`\n",
    "\n",
    "it can be loaded with `pd.read_json(filename)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454b601a",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_staff = pd.read_json(\"./data/simple.json\")\n",
    "simple_staff\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ff1402",
   "metadata": {},
   "source": [
    "But if the file is 'nested' as in contains things within things, we need to tell panda how we want it read.\n",
    "Notice that in file `nested.json` the staff members are no longer at the top level. Also that there is a List of information ('shifts') inside of each patient. It is no longer obvious how to change it into 'excel spreadsheet' format that is so easy for CSV\n",
    "\n",
    "`{\n",
    "  \"hospital\":\"Western General\",\n",
    "  \"staff\":[\n",
    "    {\"name\":\"Pim\", \"age\":34, \"department\":\"Oncology\", \"shifts\": [\"day\"]},\n",
    "    {\"name\":\"Jannet\", \"age\":54, \"department\":\"Oncology\", , \"shifts\": [\"day\", \"night\"]},\n",
    "    {\"name\":\"Aoife\", \"age\":25, \"department\":\"Oncology\",  \"shifts\": [\"night\"]}\n",
    "\t]\n",
    "}\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff02fc64",
   "metadata": {},
   "source": [
    "If we do not specify what we mean, we'll get something like this: (not very useful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbca5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_staff = pd.read_(\"./data/nested.json\")\n",
    "nested_staff\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cb0792",
   "metadata": {},
   "source": [
    "So we need to specify things. eg, staff members are nested under key `staff`.\n",
    "\n",
    "There are two ways to do this:\n",
    "\n",
    "1. Load the file, and then create another dataframe by 'interpreting' just one of the columns (the staff column in above example)\n",
    "\n",
    "This is a simple way if your data is simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b67c0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_all_data = pd.read_json(\"./data/nested.json\")\n",
    "nested_staff = pd.DataFrame.from_records(nested_all_data['staff'])\n",
    "nested_staff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ef4c04",
   "metadata": {},
   "source": [
    "2. Load the contents of the file first, and then create dataframe specifying where things come from.\n",
    "\n",
    "A) Load the content of a file into a python Dictionary\n",
    "\n",
    "B) Turn that dictionary into DataFrame specifying settings\n",
    "\n",
    "This is needed if your data is more complex and also allows you to add some meta information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579dd01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# local file:\n",
    "import json\n",
    "\n",
    "file_location = \"./data/nested.json\"\n",
    "with open(file_location) as local_data:    \n",
    "    file_data_staff = json.load(local_data) \n",
    "    \n",
    "file_data_staff # see it  \n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7ab9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# online file\n",
    "import urllib.request\n",
    "import json\n",
    "\n",
    "file_url = \"https://www.opendata.nhs.scot/api/3/action/datastore_search?resource_id=139f61d8-a87d-419d-b7af-31f555a60c89\"\n",
    "with urllib.request.urlopen(file_url) as online_data:\n",
    "    file_data_nursing_homes = json.load(online_data)\n",
    "    \n",
    "file_data_nursing_homes # see it  \n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67aa00f2",
   "metadata": {},
   "source": [
    "###  Coming back to the simpler json file:\n",
    "\n",
    "Let's specify what is where in the json:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ca8853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first let's see the data\n",
    "# list of staff is under a 'staff' key\n",
    "file_data_staff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b73b075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that we have the data as python dictionary, we can turn it into a DataFrame\n",
    "# first argument:\n",
    "# we spoecify where in dictionary is what we're after.\n",
    "\n",
    "nested_staff_df = pd.json_normalize(file_data_staff, ['staff'])\n",
    "nested_staff_df\n",
    "# if you do not have to drill to multiple levels you could also just say 'staff' without [ ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e50888c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# second argument:\n",
    "# what meta-data we also want to keep for eachrow. notice these can drill into many levels\n",
    "# here we submit a list of levels we want to keep: \n",
    "# hospital  - nodrilling, just top level\n",
    "# ['location', 'city']   - drill into location first, then into city\n",
    "nested_staff_df = pd.json_normalize(file_data_staff, ['staff'],\n",
    "                                 ['hospital', ['location', 'city']])\n",
    "nested_staff_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f68949",
   "metadata": {},
   "source": [
    "### Extra quest for the brave ones: nested items inside of json and 'dummy' binary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5b51d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally to separate items individual to each row (eg 'age') from those for whole set\n",
    "# eg. hospital, we can specify a prefix for itels from the list\n",
    "nested_staff_df = pd.json_normalize(file_data_staff, ['staff'],\n",
    "                                 ['hospital', ['location', 'city']], \n",
    "                    record_prefix='staff.')\n",
    "nested_staff_df\n",
    "# prefix can be anything, eg 'staff_', 'person-', or even nothing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d459fc3",
   "metadata": {},
   "source": [
    "Final bit of the puzzle would be untangling the staff.shifts. \n",
    "\n",
    "But this will have to be an adventure for another time. If you're curious to explore it by yourself, look for for something along the lines of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011f4297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn data into binarry dummies: 1 if present, 0 if not-present \n",
    "shifts_df = pd.get_dummies(nested_staff_df['staff.shifts'].explode()).groupby(level=0).sum()\n",
    "shifts_df = shifts_df.add_prefix('staff.shift.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6bad35",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([nested_staff_df, shifts_df], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770a84f1",
   "metadata": {},
   "source": [
    "## Real Data example\n",
    "\n",
    "Back tot he complicated online file. Let's load it again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169422ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# online file\n",
    "import urllib.request\n",
    "import json\n",
    "\n",
    "file_url = \"https://www.opendata.nhs.scot/api/3/action/datastore_search?resource_id=139f61d8-a87d-419d-b7af-31f555a60c89&limit=10000\"\n",
    "with urllib.request.urlopen(file_url) as online_data:\n",
    "    file_data_nursing_homes = json.load(online_data)\n",
    "    \n",
    "file_data_nursing_homes # see it  \n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5c90bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# where is just the list of the data? This is still a dictionary\n",
    "file_data_nursing_homes['result']['records']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b73819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now load json into a dataframe\n",
    "nursing_homes_df = pd.json_normalize(file_data_nursing_homes, ['result','records'])\n",
    "nursing_homes_df\n",
    "\n",
    "# notice it's ['result','records'] instead of ['result']['records'] \n",
    "# because at this point we are already in pandas naming world. Panda uses [fist_level, second_level, ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f9739d",
   "metadata": {},
   "source": [
    "## Folding the data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec1de89",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pivot tables - these qre are quite powerful.\n",
    "# take data and 'fold it' into a more excel like format. \n",
    "# You can apply transformations to things that got folded. eg average, or sum\n",
    "\n",
    "fruit_df = pd.DataFrame({'fruit': ['apple', 'banana','banana', 'banana', 'kiwi', 'kiwi'],\n",
    "                 'weight': [32,45,62,43,12,14]})\n",
    "\n",
    "\n",
    "avg_fruit_weights = fruit_df.pivot_table(values='weight',\n",
    "                 index='fruit',\n",
    "                 aggfunc=np.mean)\n",
    "avg_fruit_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41edbc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's another aggregate function: sum\n",
    "total_fruit_weights = fruit_df.pivot_table(values='weight',\n",
    "                 index='fruit',\n",
    "                 aggfunc=np.sum)\n",
    "total_fruit_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082bc2c6",
   "metadata": {},
   "source": [
    "### But why and how we would fold data? Long format vs Wide format\n",
    "\n",
    "In the real data above you can see that some columns are repeated many times. It's because this data has a **long format*\n",
    "\n",
    "Wide is what you know from excel - columns and rows are meaningful and contain data\n",
    "\n",
    "`\n",
    "student, math, literature\n",
    "Pin,  B, A\n",
    "Aoife, A, C\n",
    "Marta, A, B\n",
    "`\n",
    "\n",
    "Long is how you can imagine a tall thin column with metadata. All information is the same, but it it presented in a way where\n",
    "\n",
    "- first many columns tell you everythong ABOUT the data\n",
    "- final column tells you what the actual value of the data is\n",
    " \n",
    "`\n",
    "student, subject, grade\n",
    "Pim, math, B\n",
    "Pim, literature, A\n",
    "Aoife, math, A\n",
    "Aoife, literature, C\n",
    "Marta, math, A\n",
    "Marta, literature, B\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c916d52",
   "metadata": {},
   "source": [
    "So let's look at the nursing home data. They are currently Long. With 'meta' columns being: `Date, KeyStatistic, CA, MainClientGroup`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7d2043",
   "metadata": {},
   "outputs": [],
   "source": [
    "nursing_homes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96770af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is quite an advanced thing to try, but you can 'fold' the data using some columns\n",
    "# eg. if we want a breakdown for each date\n",
    "# where columns are KeyStatistic\n",
    "# and each cell shows the Value\n",
    "# and in case there are many items which would go into each cell, get the numpy.average\n",
    "\n",
    "nursing_homes_df_wide=pd.pivot_table(nursing_homes_df, \n",
    "                               index=['Date'], \n",
    "                               columns = 'KeyStatistic',\n",
    "                               aggfunc = np.average,\n",
    "                               values = 'Value')\n",
    "\n",
    "nursing_homes_df_wide.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27adeb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# but if you also wanted to see how this breakdown works across council area\n",
    "nursing_homes_df_wide=pd.pivot_table(nursing_homes_df, \n",
    "                               index=['CA','Date'], \n",
    "                               columns = 'KeyStatistic',\n",
    "                               aggfunc = np.average,\n",
    "                               values = 'Value')\n",
    "\n",
    "nursing_homes_df_wide.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc99046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and if needed you can then turn it back into long\n",
    "columns_to_melt=list(nursing_homes_df_wide.columns)\n",
    "nursing_homes_df_long_again = pd.melt(nursing_homes_df_wide, \n",
    "                                      value_vars=columns_to_melt,\n",
    "                                      value_name='Age',  # here you can call your value column, eg 'Value' \n",
    "                                      ignore_index=False)\n",
    "nursing_homes_df_long_again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd60d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We might look more at folding things betwene Long and Wide later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0198fb3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43072491",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86faab5c",
   "metadata": {},
   "source": [
    "# Finding and Fixing Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2f5abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning up missing data (with NaN values)\n",
    "\n",
    "foods = pd.DataFrame({\n",
    "    'name': ['Bagel', 'Milk Chocolate', 'Carrot', 'Ham Sandwich', 'Egg Cake', 'Tap Water'],\n",
    "    'diet':['Vegan', None, 'Vegan', 'Meat', None, 'Vegan'],\n",
    "    'supplier':['Bros', 'Luca', 'Whitmore', 'Union', 'Lovecrumbs', None],\n",
    "    'price':[4.30, 2.10, 0.7, 5.70, None, 0]\n",
    "}).set_index('name', drop=True)\n",
    "\n",
    "print(\"\\nShape:\",foods.shape)\n",
    "\n",
    "print(\"\\nMissing values:\\n\",foods.isnull().sum())\n",
    "\n",
    "print(\"\\nAll values:\\n\",foods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350f985e",
   "metadata": {},
   "outputs": [],
   "source": [
    "foods = pd.DataFrame({\n",
    "    'name': ['Bagel', 'Milk Chocolate', 'Carrot', 'Ham Sandwich', 'Egg Cake', 'Tap Water'],\n",
    "    'diet':['Vegan', None, 'Vegan', 'Meat', None, 'Vegan'],\n",
    "    'supplier':['Bros', 'Luca', 'Whitmore', 'Union', 'Lovecrumbs', None],\n",
    "    'price':[4.30, 2.10, 0.7, 5.70, None, 0]\n",
    "}).set_index('name', drop=True)\n",
    "\n",
    "# remove all rows with any missing values \n",
    "foods.dropna(inplace=True)\n",
    "\n",
    "print(\"\\nShape:\",foods.shape)\n",
    "\n",
    "print(\"\\nMissing values:\\n\",foods.isnull().sum())\n",
    "\n",
    "print(\"\\nAll values:\\n\",foods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c8532d",
   "metadata": {},
   "outputs": [],
   "source": [
    "foods = pd.DataFrame({\n",
    "    'name': ['Bagel', 'Milk Chocolate', 'Carrot', 'Ham Sandwich', 'Egg Cake', 'Tap Water'],\n",
    "    'diet':['Vegan', None, 'Vegan', 'Meat', None, 'Vegan'],\n",
    "    'added_sugar':[True, True, False, False,  True, False],\n",
    "    'supplier':['Bros', 'Luca', 'Whitmore', 'Union', 'Lovecrumbs', None],\n",
    "    'price':[4.30, 2.10, 0.7, 5.70, None, 0]\n",
    "}).set_index('name', drop=True)\n",
    "\n",
    "# Cleanup the Diet column:\n",
    "\n",
    "print(\"missing values in Diet:\", foods.diet.isna().sum())\n",
    "print(\"present values in Diet:\\n\", foods.diet.value_counts(sort=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9039d479",
   "metadata": {},
   "outputs": [],
   "source": [
    "foods.diet.fillna('Unknown',inplace=True)\n",
    "print()\n",
    "\n",
    "print(\"missing values in Diet:\", foods.diet.isna().sum())\n",
    "print(\"present values in Diet:\\n\", foods.diet.value_counts(sort=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49b75e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice unknown string values show up as None, while missing numeric values show up as Unknown.\n",
    "# and keep cleaning up columns until there are no NaNs in all columns\n",
    "print(foods.isnull().sum() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1b38f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "foods # still some NaN in price and supplier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aab9684",
   "metadata": {},
   "outputs": [],
   "source": [
    "foods.price.fillna(0,inplace=True) # cleanup price\n",
    "foods.supplier.fillna(\"Unknown\",inplace=True) # cleanup supplier\n",
    "print(foods.isnull().sum() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3f4889",
   "metadata": {},
   "outputs": [],
   "source": [
    "foods # no more NaNs - they were all replaced by meaningful values like 'Unknown' or 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2641fb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some simple statistics on distribution of values\n",
    "foods.diet.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffc783b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# or as fraction of all items\n",
    "foods.diet.value_counts()/foods.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8404b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# or as percent of all items\n",
    "np.round( foods.diet.value_counts()/foods.shape[0]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fcc6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# or as a breakdown to all possible values, between two categories (wide)\n",
    "pd.crosstab(foods.added_sugar, foods.diet, margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce31f6f",
   "metadata": {},
   "source": [
    "# Visualisation\n",
    "\n",
    "There are many visualisation libraries, but in this course we will use Plotly, and sometimes matplotlib. Some might be familiar to you from R "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28880f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "pio.renderers.default='notebook'\n",
    "import pprint as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0acc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "foods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e326e975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "fig = go.Figure(\n",
    "    data=[go.Bar(y=foods.price, \n",
    "                 x=foods.index)],\n",
    "    layout=go.Layout(\n",
    "        title=go.layout.Title(text=\"Prices of foods\")\n",
    "    )\n",
    ")\n",
    "fig.show('notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1dad32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "diet_colours = {'Vegan':'green',\n",
    "                'Meat':'red',\n",
    "                'Vegetarian':'orange',\n",
    "                'Unknown':'grey',\n",
    "               }\n",
    "\n",
    "\n",
    "foods['color'] = foods['diet'].map( lambda diet: diet_colours[diet] )\n",
    "\n",
    "fig = go.Figure(\n",
    "    data=[go.Bar(y=foods.price, \n",
    "                 x=foods.index, \n",
    "                 marker_color=foods.color)],\n",
    "    layout=go.Layout(\n",
    "        title=go.layout.Title(text=\"Prices of foods\"),\n",
    "        yaxis_title=\"Price in Pounds\"\n",
    "    )\n",
    ")\n",
    "fig.show('notebook')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7475cf6b",
   "metadata": {},
   "source": [
    "### In matplotlib it looks very simmilar:\n",
    "\n",
    "matplot lib is very popular in many languages, but needs more care to look great."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88170a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "    \n",
    "plt.bar(foods.index, \n",
    "        foods.price,\n",
    "        color=foods.color)\n",
    "plt.ylabel('Price in Pounds')\n",
    "plt.title('Prices of foods')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164f7773",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c819d314",
   "metadata": {},
   "source": [
    "## Now with some real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e138ec4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nursing_homes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73115994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick only the Mean Age statistic\n",
    "nursing_homes_df.loc[nursing_homes_df['KeyStatistic']=='Mean Age']\n",
    "\n",
    "# group values by Date to get overall averages\n",
    "averages = nursing_homes_df['Value'].groupby(nursing_homes_df['Date']).mean()\n",
    "print(type(averages)) # what type is averages? it's a series. So basically one column of a dataframe\n",
    "averages                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51753a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's turn it into a dataframe, fo ease of use\n",
    "averages_df = pd.DataFrame(averages)\n",
    "averages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273e0cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(\n",
    "    data=[go.Scatter(y=averages_df.Value, \n",
    "                 x=averages_df.index)]\n",
    ")\n",
    "fig.show('notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e6a09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do you see what is going wroing with the above graph? (on the x axis). \\\n",
    "# That's because the x axis values are numbers like 20201029 instead of actual datetime objects\n",
    "# let's fix that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87829dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "averages_df = pd.DataFrame(averages)\n",
    "averages_df.index = pd.to_datetime(averages_df.index, format='%Y%m%d')\n",
    "averages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311d6a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(\n",
    "    data=[go.Scatter(y=averages_df.Value, \n",
    "                 x=averages_df.index)]\n",
    ")\n",
    "fig.show('notebook')\n",
    "# notice that the x axis suddently makes sense!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae024ffa",
   "metadata": {},
   "source": [
    "# And to sum it up: let's try to use more than one part of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773de4e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c90fbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe\n",
    "stats_df = pd.DataFrame(nursing_homes_df['Value'].groupby(nursing_homes_df['Date']).mean())\n",
    "# then add to if a few more columns. for example 1 standard deviation up and down from mean.\n",
    "stats_df['Median'] = nursing_homes_df['Value'].groupby(nursing_homes_df['Date']).median()\n",
    "stats_df['Stdev'] = nursing_homes_df['Value'].groupby(nursing_homes_df['Date']).std()\n",
    "stats_df['Stdev_top'] = stats_df['Value']  + stats_df['Stdev'] \n",
    "stats_df['Stdev_bottom'] = stats_df['Value']  - stats_df['Stdev'] \n",
    "\n",
    "stats_df['Max'] = nursing_homes_df['Value'].groupby(nursing_homes_df['Date']).max()\n",
    "stats_df['Min'] =nursing_homes_df['Value'].groupby(nursing_homes_df['Date']).min()\n",
    "\n",
    "# and let's make dates dates!\n",
    "stats_df.index = pd.to_datetime(stats_df.index, format='%Y%m%d')\n",
    "stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527555f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = nursing_home_ages_mean_overall_ca.merge(ca_lookup_simple, left_on = 'CA', right_on = 'CA')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6b6560",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(\n",
    "    data=[go.Scatter(y=stats_df.Value, x=stats_df.index, name=\"Mean\"),\n",
    "         go.Scatter(y=stats_df.Stdev_bottom, x=stats_df.index, name=\"1 standard dev down\"),\n",
    "         go.Scatter(y=stats_df.Median, x=stats_df.index, name=\"Median\"),\n",
    "         go.Scatter(y=stats_df.Stdev_top, x=stats_df.index, name=\"1 standard dev up\")]\n",
    ")\n",
    "fig.show('notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27a8f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll continue this work during the lab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804f17ea",
   "metadata": {},
   "source": [
    "## ⭐️⭐️⭐️💥 What you learned in this session: Three stars and a wish \n",
    "**In your own words** write in your Learn diary:\n",
    "\n",
    "- 3 things you yould like to remember from this badge\n",
    "- 1 thing you wish to understand better in the future or a question you'd like to ask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eeb1f3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
