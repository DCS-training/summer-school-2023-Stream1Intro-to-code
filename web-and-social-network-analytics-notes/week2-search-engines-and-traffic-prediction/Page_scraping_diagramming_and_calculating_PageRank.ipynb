{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this at the beginning of the class to load all dependancies \n",
    "# (you can run things with Shift+Enter keyboard shortcut, or with the RUN button above)\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import pprint as pp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this notebook\n",
    "\n",
    "In this short notebool we will learn how to scrape links from a website. (Initially just a simple website where we load it as HTML with beautiful soup, with no need to Selenium chrome-pupeteering)\n",
    "\n",
    "Then we will visit all the websites these links point to... and gather links from them. This way we will create a frontier and crawl/spider our way through the website and create its map.\n",
    "\n",
    "Then we will represent that map as a simple graph.\n",
    "\n",
    "And then we will scale the graph accordin to it's PageRank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But before we start: Map all the links of a website by hand. \n",
    "\n",
    "Visit all pages in the demo folder and on paper create a map of all the links\n",
    "\n",
    "To view the same pages as a website: run the code below and open the link.\n",
    "\n",
    "NOTE: if you just click the html files from the list, they will open, but all the links will not work (you will see a 403 error). So instead use below code\n",
    "\n",
    "When you are viewing a page, remember that you can hover over a link with your mouse and you will see where that link would take you if you click it. (it usually appears on the bottom right of your web browser)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell will print a link for you to follow:\n",
    "folder_with_app_pages = \"file:///\" + os.getcwd() + \"/demowebsite/\"\n",
    "starting_website = folder_with_app_pages + \"home.html\"\n",
    "\n",
    "print(\"copy this link into another tab in your browser and visit it:\")\n",
    "\n",
    "print(starting_website)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spend a minute drawing all the links, only by clicking in your web browser. You will end up with a spider-web style diagram. Quite often you will have to click the 'back' button, to get back to the page.\n",
    "\n",
    "### Questions:\n",
    "\n",
    "- Can you reach all the html files in the folder? \n",
    "- Are some pages dead ends? (you can't click to anywhere from them)\n",
    "- What page is most pointed to? And what page points to the most other pages?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's try to automate that process. \n",
    "\n",
    "We will\n",
    "\n",
    "- visit the first page and store all the links we find in it\n",
    "- visit all these links in turn. But only if we have not visited them already (to avoid an infinite loop)\n",
    "\n",
    "We'll need a function that can visit a website and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visit_page_and_return_links(page_url):\n",
    "    page_url_in_a_folder  = \"file:///\" + os.getcwd() + \"/demowebsite/\" + page_url\n",
    "    print(f\"Looking for links in {page_url_in_a_folder}\")\n",
    "    \n",
    "    html_of_website = urlopen(page_url_in_a_folder)\n",
    "    soup = BeautifulSoup(html_of_website, 'html.parser')\n",
    "\n",
    "    # .find_all links\n",
    "    links = soup.find_all('a')\n",
    "    link_urls = []\n",
    "    \n",
    "    # remember, links look like this: <a href=\"team.html\">Team</a>\n",
    "    # but we only care about the content of href attribute (href == 'html reference')\n",
    "    for link in links:\n",
    "        print('complete html of link: ', link, \"\\t points to \", link['href'])\n",
    "        #  add every link you have to the output list       \n",
    "        link_urls.append(link['href'])\n",
    "    # return output list with all urls          \n",
    "    return link_urls\n",
    "        \n",
    "    \n",
    "starting_website = \"home.html\"\n",
    "    \n",
    "found_links = visit_page_and_return_links(starting_website)\n",
    "print(\"Found links:\", found_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can write a loop that will visit all the pages. And when it visits them, will keep track of where they point. This could be done with classes, dictionaries or numpys. We'll use a list of dictionaries, but feel free to translate it into numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when we are done, each page info will look like this:\n",
    "demo_page = {\"address\":\"home.html\", \"links_to\": ['team.html', 'news.html', 'business_deals.html', 'shop.html'] }\n",
    "\n",
    "pp.pprint(demo_page) #note pp.pprint() is a 'pretty version of print'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And we will have a list of these dictionaries, like\n",
    "demo_all_pages = [{'address': 'team.html', 'links_to': []},\n",
    "             {'address': 'home.html',\n",
    "             'links_to': ['team.html', 'news.html', 'business_deals.html', 'shop.html']}]\n",
    "\n",
    "pp.pprint(demo_all_pages) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's visit our starting page, but first we'll make our function return above format of data\n",
    "\n",
    "def visit_page_and_return_dictionary(page_url):\n",
    "    #  this part is as before:   \n",
    "    page_url_in_a_folder  = \"file:///\" + os.getcwd() + \"/demowebsite/\" + page_url\n",
    "    print(f\"Looking for links in {page_url}\")\n",
    "    html_of_website = urlopen(page_url_in_a_folder)\n",
    "    soup = BeautifulSoup(html_of_website, 'html.parser')\n",
    "    links = soup.find_all('a')\n",
    "    link_urls = []\n",
    "    for link in links:    \n",
    "        link_urls.append(link['href'])\n",
    "        \n",
    "    # this is new: do not just return a list. rather, returned a structured page info     \n",
    "    return {'address': page_url, \n",
    "            'links_to': link_urls}\n",
    "    \n",
    "starting_website = \"home.html\"\n",
    "page_info = visit_page_and_return_dictionary(starting_website)\n",
    "print()\n",
    "pp.pprint(page_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will start two lists: Pages we have visited, and pages we have not visited yet:\n",
    "# note that there are countless ways to do that.\n",
    "\n",
    "starting_website = \"home.html\"\n",
    "\n",
    "pages_we_visited = []\n",
    "pages_to_visit = [starting_website] # this is new. LEt's add starting page to the pages_to_visit at first\n",
    "pages_scraped_info = []\n",
    "\n",
    "next_page_to_visit = pages_to_visit.pop() # we grab and remove the first page to visit\n",
    "page_info = visit_page_and_return_dictionary(next_page_to_visit)\n",
    "pages_scraped_info.append(page_info)\n",
    "\n",
    "print(\"page_info\", page_info)\n",
    "\n",
    "pages_we_visited.append(page_info['address']) # we visited this page\n",
    "\n",
    "# for all links, if they were not yet visited, add them to pages_to_visit\n",
    "for link_url in page_info['links_to']:\n",
    "    if link_url not in pages_we_visited:\n",
    "        pages_to_visit.append(link_url)\n",
    "        \n",
    "print()\n",
    "print(\"pages_we_visited\", pages_we_visited)\n",
    "print(\"pages_to_visit\", pages_to_visit)\n",
    "print(\"pages_scraped_info\", pages_scraped_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now all we need to do is to repeat this process while there are any pages left in pages_to_visit\n",
    "\n",
    "# this is the same\n",
    "starting_website = \"home.html\"\n",
    "pages_we_visited = []\n",
    "pages_to_visit = [starting_website] \n",
    "pages_scraped_info = []\n",
    "\n",
    "# keep repeating your code, until there are no more pages to visit\n",
    "while len(pages_to_visit) > 0:\n",
    "    # rest is the same\n",
    "    next_page_to_visit = pages_to_visit.pop() \n",
    "    page_info = visit_page_and_return_dictionary(next_page_to_visit)\n",
    "    pages_scraped_info.append(page_info)\n",
    "\n",
    "    pages_we_visited.append(page_info['address']) \n",
    "    for link_url in page_info['links_to']:\n",
    "        if link_url not in pages_we_visited:\n",
    "            pages_to_visit.append(link_url)\n",
    "        \n",
    "\n",
    "        \n",
    "print()\n",
    "print(\"pages_we_visited\", pages_we_visited)\n",
    "print(\"pages_to_visit\", pages_to_visit)\n",
    "print(\"pages_scraped_info\")\n",
    "pp.pprint(pages_scraped_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is this consistent with your notes that you created while manually clicking on page links?\n",
    "\n",
    "When there are just a few pages, this process can be done manually, but as soon as you want to with no errors, and at scale (whcih is basically, always)... it is better to use software to do it for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drawing a simple graph\n",
    "\n",
    "We will talk about graphs a bit more next week, but meanwhile I will show you a very quick one. We will use the data from our exercise above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a graph\n",
    "graph = nx.DiGraph()\n",
    "\n",
    "# add some edges\n",
    "graph.add_edge('A','B')\n",
    "graph.add_edge('A','D')\n",
    "graph.add_edge('A','C')\n",
    "graph.add_edge('C','B')\n",
    "\n",
    "# calculate 'elastic' layout\n",
    "positions = nx.spring_layout(graph)\n",
    "\n",
    "# draw the graph (you migh see a pink warning the first time. just ignore it)\n",
    "nx.draw(graph, positions, with_labels=True)\n",
    "plt.show()\n",
    "\n",
    "# notice tiny arrows indicating where graphs come from and go to:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use pages_scraped_info to create a graph\n",
    "\n",
    "Basically, for each page, we will add edges (arrows) to all the pages the page has links to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = nx.DiGraph()\n",
    "\n",
    "# for every page we scraped\n",
    "for page in pages_scraped_info:\n",
    "    # take link origin (basically, that page)\n",
    "    link_origin = page['address']\n",
    "    all_links = page['links_to']\n",
    "    # and for all pages it had links to (basically, the destinations)\n",
    "    for link_destination in all_links:\n",
    "        # create graph adges pointing from origin to destination\n",
    "        graph.add_edge(link_origin, link_destination)\n",
    "\n",
    "positions = nx.spring_layout(graph)\n",
    "nx.draw(graph, positions, with_labels=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's draw it again with bigger blobs:\n",
    "\n",
    "graph = nx.DiGraph()\n",
    "\n",
    "for page in pages_scraped_info:\n",
    "    link_origin = page['address']\n",
    "    all_links = page['links_to']\n",
    "    for link_destination in all_links:\n",
    "        graph.add_edge(link_origin, link_destination)\n",
    "\n",
    "positions = nx.spring_layout(graph)\n",
    "nx.draw(graph, positions, with_labels=True,  node_size = 1000) # here you can set the size\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are these arrows consistent with your notes that you created while manually clicking on page links?\n",
    "\n",
    "Can you see how it could be useful to map and represent someone's web page automatically like that? And analyse it? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "\n",
    "We'll talk a bit more about metrics next week, but here is a simple measure of PageRank. We could calculate it by hand, but it would be rather time consuming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pageranks = nx.pagerank(graph)\n",
    "\n",
    "pp.pprint(pageranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions = nx.spring_layout(graph)\n",
    "\n",
    "size = [pagerank * 5000 for pagerank in pageranks.values()]\n",
    "\n",
    "nx.draw(graph, positions, with_labels= True, node_size = size)\n",
    "plt.title(\"Size scaled to PageRank\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Think about it: \n",
    "\n",
    "- What can you say about this website, knowing all the links and their page ranks? \n",
    "- Is this a good outcome? Is that there you would like your users to be?\n",
    "- how would you change the links structure to e.g. increase amount of predicted traffic to business_deals.html ?\n",
    "\n",
    "\n",
    "# Mini Task:\n",
    "\n",
    "If you feel adventurous try to implement your suggestions from above - edit the html files to add/remove links, and then re-run this notebook from the top. Did you achieve the task you wanted to?\n",
    "\n",
    "###Â Note about editing htmls files in jupyter:\n",
    "\n",
    "If you'd like to view and edit html of these pages, you can go to your list of files in Jupyter. In there in the folder called `demowebsite` you will find a set of html pages: **to view and edit them as html: tick the ckeckbox next to a file and select 'edit' button on top of the screen**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
