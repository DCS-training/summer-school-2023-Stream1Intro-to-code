{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this at the beginning of the class to load all dependancies \n",
    "# (you can run things with Shift+Enter keyboard shortcut, or with the RUN button above)\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import pprint as pp\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessment Preparation Challenge\n",
    "\n",
    "- These challenges are optional, are not marked and are just for you. \n",
    "- But they will prepare you for the course's final assesment. If you complete all of the challenges, you will have a much easier time doing the final assesment.\n",
    "- you will not be given answers to them, because large parts of the assesment will be very simmilar and if we gave you the answers you could copy-paste parts of it for assesment which would defeat the purpose of the assesment :)\n",
    "\n",
    "\n",
    "### Challange 2: Modify the code from this week's notebook to crawl an actual online website, not just files on your computer.\n",
    "\n",
    "You will likely want to change these aspects:\n",
    "\n",
    "- use simple websites: some websites, like bbc.co.uk use a lot of javascript, and take time to load. This means that you might not be able to scrape them by just asking for their html pages with urlopen. Do not look at complex pages, or (as a rather difficult exercise) use selenium.\n",
    "- you might want to limit scraping pages just to those within your website, otherwise a link to twitter will take you to twitter, and you start scraping completely unrelated sources. You could achieve that with an if statment right after `link_address = link.get('href', default = link)`\n",
    "- limit amount of times your loop will run. Otherwise you might keep going forever. do something like this inside of your while loop: \n",
    "\n",
    "`if len(visited_pages) > 20:\n",
    "    break`\n",
    "    \n",
    "or even\n",
    "\n",
    "`while len(pages_to_visit) > 0 and len(pages_we_visited) < 20:`\n",
    "\n",
    "\n",
    "\n",
    "For fun, try a few, preferably small web pages without too many links\n",
    "\n",
    "- https://www.edinburghshogmanay.com/\n",
    "- https://edinburghcyclehire.com/\n",
    "\n",
    "# below you will find the code from the lesson it is primarilly already modified for you. You might need to tweak to your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's visit our starting page, but first we'll make our function return above format of data\n",
    "\n",
    "def domain_only(full_url):\n",
    "    without_https = full_url.lstrip(\"https\").lstrip(\"://\")\n",
    "    end_of_domain = without_https.find(\"/\") if \"/\" in without_https else len(without_https)\n",
    "    return without_https[0:end_of_domain]\n",
    "\n",
    "assert domain_only(\"https://bbc.co.uk/\") == \"bbc.co.uk\"\n",
    "assert domain_only(\"https://bbc.co.uk\") == \"bbc.co.uk\"\n",
    "assert domain_only(\"https://bbc.co.uk/news\") == \"bbc.co.uk\"\n",
    "assert domain_only(\"http://twitter.com/account\") == \"twitter.com\"\n",
    "print(\"tests passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def protocol_only(full_url):\n",
    "    if \"https://\" in full_url:\n",
    "        return \"https://\" \n",
    "    elif \"http://\" in full_url:\n",
    "        return \"http://\" \n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "assert protocol_only(\"https://bbc.co.uk/\") == \"https://\"\n",
    "assert protocol_only(\"http://bbc.co.uk\") == \"http://\"\n",
    "assert protocol_only(\"bbc.co.uk/news\") == \"\"\n",
    "print(\"tests passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_the_link(page_url, link_address, keep_external_links = False):\n",
    "    if \"http\" not in link_address: \n",
    "        protocol = link_address\n",
    "        \n",
    "        #  internal links: if a website https://bbc.co.uk/weather has a link to /news it  means bbc.co.uk/news \n",
    "        #  so grab domain and add link at the end\n",
    "        link_address = protocol_only(page_url) + domain_only(page_url) + link_address\n",
    "        \n",
    "    else:\n",
    "        # only accept external links within this domain, so from https://bbc.co.uk only accept other bbc links\n",
    "        # to do that strip the https:// and then grab everything before first /\n",
    "        if domain_only(link_address) != domain_only(page_url) and keep_external_links == False:\n",
    "            link_address = page_url\n",
    "    \n",
    "    if \"mailto\" in link_address: # also ignore \"mailto\" email links\n",
    "        link_address = page_url\n",
    "    \n",
    "    if \"#\" in link_address: # also trim the #id at the end (it tells browser to scroll down)\n",
    "        link_address = link_address[0:link_address.find(\"#\")]\n",
    "    if \"?\" in link_address: # also trim the #id at the end (it tells browser to scroll down)\n",
    "        link_address = link_address[0:link_address.find(\"?\")]\n",
    "        \n",
    "    link_address = link_address.rstrip(\"/\") # cleanup trailing /\n",
    "        \n",
    "    return link_address\n",
    "\n",
    "# if link it to an external website, just replace it with the link to the page it came from\n",
    "assert cleanup_the_link(\"https://bbc.co.uk/\", \"/news\") == \"https://bbc.co.uk/news\"\n",
    "assert cleanup_the_link(\"https://bbc.co.uk\", \"/news\") == \"https://bbc.co.uk/news\"\n",
    "assert cleanup_the_link(\"https://bbc.co.uk/news\", \"/weather\") == \"https://bbc.co.uk/weather\"\n",
    "assert cleanup_the_link(\"https://twitter.com\", \"https://bbc.co.uk/\") == \"https://twitter.com\"\n",
    "assert cleanup_the_link(\"https://twitter.com\", \"https://bbc.co.uk/\", True) == \"https://bbc.co.uk\"\n",
    "\n",
    "\n",
    "assert cleanup_the_link(\"https://bbc.co.uk/news\", \"/weather#footer\") == \"https://bbc.co.uk/weather\"\n",
    "assert cleanup_the_link(\"https://bbc.co.uk/news\", \"/mailto:info@bbc.co.uk\") == \"https://bbc.co.uk/news\"\n",
    "assert cleanup_the_link(\"https://bbc.co.uk/news\", \"/weather?city=edinburgh\") == \"https://bbc.co.uk/weather\"\n",
    "print(\"tests passed\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visit_page_and_return_dictionary(page_url, keep_external_links = False):\n",
    "    #     here I removed a dependency on local holders\n",
    "    \n",
    "    link_urls = []\n",
    "    print(f\"Looking for links in {page_url}\")\n",
    "    \n",
    "    # try catch is there to prtext you from broken links, or links to non-html things, like .pdf .mp3   \n",
    "    try:\n",
    "        # this part is new, and required for https:// pages (with security SSL padlock)  \n",
    "        html_of_website = requests.get(page_url).content.decode()    \n",
    "    except:\n",
    "        # if something went wrong, just pretend there were no links.        \n",
    "        return {'address': page_url, \n",
    "                'links_to': []}\n",
    "    \n",
    "    soup = BeautifulSoup(html_of_website, 'html.parser')\n",
    "    links = soup.find_all('a')\n",
    "    for link in links:\n",
    "        link_url = link.get('href', default = page_url)\n",
    "        link_url = cleanup_the_link(page_url, link_url, keep_external_links)\n",
    "        \n",
    "        link_urls.append(link_url)\n",
    "        \n",
    "    link_urls = list(set(link_urls)) # remove duplicates\n",
    "        \n",
    "    return {'address': page_url, \n",
    "            'links_to': link_urls}\n",
    "    \n",
    "visit_page_and_return_dictionary(\"https://www.marysmilkbar.com/yourvisit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_website(starting_website, keep_external_links = False):\n",
    "    pages_we_visited = []\n",
    "    pages_to_visit = [starting_website] \n",
    "    pages_scraped_info = []\n",
    "\n",
    "    while len(pages_to_visit) > 0 and len(pages_we_visited) < 20: # for now only crawl 10 links\n",
    "        next_page_to_visit = pages_to_visit.pop() \n",
    "        page_info = visit_page_and_return_dictionary(next_page_to_visit, keep_external_links)\n",
    "        pages_scraped_info.append(page_info)\n",
    "\n",
    "        pages_we_visited.append(page_info['address']) \n",
    "        for link_url in page_info['links_to']:\n",
    "            if link_url not in pages_we_visited:\n",
    "                pages_to_visit.append(link_url)\n",
    "\n",
    "        pages_to_visit = list(set(pages_to_visit)) # remove duplicates\n",
    "\n",
    "    \n",
    "#     print()\n",
    "#     print(\"pages_we_visited\", pages_we_visited)\n",
    "#     print(\"pages_to_visit\", pages_to_visit)\n",
    "#     print(\"pages_scraped_info\")\n",
    "#     pp.pprint(pages_scraped_info)\n",
    "    return pages_scraped_info\n",
    "    \n",
    "scraped_pages = analyse_website(\"https://www.marysmilkbar.com/\")\n",
    "pp.pprint(scraped_pages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_site_info(pages_info):\n",
    "    graph = nx.DiGraph()\n",
    "\n",
    "    for page in pages_info:\n",
    "        link_origin = page['address']\n",
    "        all_links = page['links_to']\n",
    "        for link_destination in all_links:\n",
    "            graph.add_edge(link_origin, link_destination)\n",
    "\n",
    "    positions = nx.spring_layout(graph)\n",
    "    nx.draw(graph, positions, with_labels=True) \n",
    "    plt.show()\n",
    "\n",
    "graph_site_info(scraped_pages)\n",
    "\n",
    "# most likely you will see a very messy graph. This type of data requires a serious cleanup to be meaningful.\n",
    "# think how you could clean up this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try some other businesses:\n",
    "\n",
    "pages_data = analyse_website(\"https://www.soderberg.uk/\")\n",
    "graph_site_info(pages_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# but notice what happens when you allow crawling external links\n",
    "\n",
    "pages_data = analyse_website(\"https://www.soderberg.uk/\", True)\n",
    "graph_site_info(pages_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
